training:
  num_epochs: 30
  lr: 0.001
  batch_size: 128
  random_seed: 42
  device: "cuda" # or "cpu"
  log_dir: "logs/resnet/attention_transfer"
  dataset: "cifar10"
  num_workers: 4
  pin_memory: true
  experiment_name: "attention_transfer_resnet/train"
  model_dir: "src/results"
  model_path: "attention_transfer"
  augmentation: false
  # Knowledge distillation
  weight: 100. # default: 1.0
  kd_pow: 2
  kd_type: "sum" # or "max"

teacher:
  in_channels: [64, 64, 64, 64]
  layers: [3, 6, 4, 3]
  save_dir: "src/results/teacher_resnet.pth"

student:
  in_channels: [64, 64, 64, 64]
  layers: [1, 1, 1, 1]