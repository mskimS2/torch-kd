training:
  num_epochs: 30
  lr: 0.001
  batch_size: 128
  random_seed: 42
  device: "cuda" # or "cpu"
  log_dir: "logs/resnet/logits"
  dataset: "cifar10"
  num_workers: 4
  pin_memory: true
  experiment_name: "logits_resnet/train"
  model_dir: "src/results"
  model_path: "logits"
  augmentation: false
  # Knowledge distillation
  weight: 1. # default: 1.0 

teacher:
  in_channels: [64, 64, 64, 64]
  layers: [3, 6, 4, 3]
  save_dir: "src/results/teacher_resnet.pth"

student:
  in_channels: [64, 64, 64, 64]
  layers: [1, 1, 1, 1]